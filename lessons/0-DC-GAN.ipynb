{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import functools\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "\n",
    "def plot_model(model):\n",
    "    return SVG(keras.utils.vis_utils.model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review: Building a deep convolutional classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prep the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that the range of pixels is 0-255. \n",
    "# We'll go ahead and scale it from -1 to 1\n",
    "# This will be important later\n",
    "print(\"original range of pixels\", x_train.min(), x_train.max())\n",
    "\n",
    "x_train = (2/255)*x_train - 1\n",
    "x_test = (2/255)*x_test - 1\n",
    "\n",
    "print(\"new range of pixels\", x_train.min(), x_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the classifier\n",
    "\n",
    "Here, we're going to use several convolutional layers to summarize the inputs in steps. Then we'll ask the classifier to pick which class the image belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = functools.partial(keras.layers.Conv2D, filters=50, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside: We're going to be using several similar layers. Instead of writing out `keras.layers.Conv2D(50, 3, strides=2, activation=\"relu\", padding=\"same\")` several times, we can use the `functools` package to wrap the `keras.layers.Conv2D` function with the parameters we'll be using over and over. Then we can just change what we need to each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=x_train.shape[1:]),\n",
    "    \n",
    "    conv(),\n",
    "    conv(),\n",
    "    conv(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "    \n",
    "])\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer=keras.optimizers.Adam(), loss=\"sparse_categorical_crossentropy\", metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(x=x_train, y=y_train, validation_data=(x_test, y_test), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Improve Classifier\n",
    "\n",
    "(15 minutes) If time permits, see if you can improve the above classifier without significantly increasing the training time. If we don't get to this, students are encouraged to attempt this at home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=x_train.shape[1:]),\n",
    "    \n",
    "\n",
    "    keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "    \n",
    "])\n",
    "classifier.compile(optimizer=keras.optimizers.Adam(), loss=\"sparse_categorical_crossentropy\", metrics=['acc'])\n",
    "classifier.fit(x=x_train, y=y_train, validation_data=(x_test, y_test), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From classifiers to Generative Adversarial Networks\n",
    "\n",
    "Below is the typical Deep Convolutional Classifier structure:\n",
    "\n",
    "```\n",
    "Dimensions     Layer Type\n",
    "----------     ----------\n",
    "[32 x 32 x 3]  Input \n",
    "               ... <preprocessing layers> ...\n",
    "[16 x 16 x 50] Convolution (+ BN + Pooling + Strides) \n",
    "               ... <multiple conv layers> ...\n",
    "[ 4 x  4 x 50] Convolution (+ BN + Pooling + Strides) \n",
    "               ... <flatten or global pool layers> ...\n",
    "[10]           Dense\n",
    "```\n",
    "\n",
    "As you saw above, the convolution layers serve to process the input data through several steps. Above, we used a stride of two to shrink the height and width of each layer as we approached the output. Pooling can be used in place of strided convolution. \n",
    "\n",
    "Ultimately, we want to give the network the framework for processing and summarizing the input data in steps as it prepares to make a prediction.\n",
    "\n",
    "To build a GAN, we need to build two separate networks: a **discriminator** and a **generator**. \n",
    "\n",
    "The discriminator will function much like the classifiers we've just built. It will take an image as input and make a class prediction. The difference is that now, the discriminator will be classifying each image as either \"real\" or \"fake\". \n",
    "\n",
    "The generator will be a bit different. We want it to generate images that are *similar* to the training data. Now, consider the Generator's task. We could set it up to learn the **single image** (whatever that looks like) that is most similar to the 60,000 images in our training data. But if that's what the generator tried, the discriminator could easily catch it. Instead, we want the generator to produce **random images** so that each individual image looks like it could be from the training data. Then as long as our discriminator doesn't memorize the training data, the generator can fool it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_labels = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse',\n",
    "    'ship', 'truck'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = imshow((x_train[0,...]+1)/2)\n",
    "fig.axes.get_xaxis().set_visible(False)\n",
    "fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_side = 10\n",
    "im_size = 32\n",
    "\n",
    "frog_i,_ = np.where(y_train==6)\n",
    "frog_x = x_train[frog_i,...]\n",
    "frog_x = frog_x[:sample_side*sample_side]\n",
    "\n",
    "# Rescale images 0 - 255\n",
    "frog_x = (frog_x+1)/2\n",
    "\n",
    "frog_x = frog_x.reshape(sample_side,sample_side*im_size,im_size,3)\n",
    "frog_x = frog_x.transpose((0,2,1,3))\n",
    "frog_x = frog_x.reshape(sample_side*im_size,sample_side*im_size,3)\n",
    "frog_x = frog_x.transpose((1,0,2))\n",
    "\n",
    "figsize(10, 10)\n",
    "\n",
    "fig = imshow(frog_x)\n",
    "fig.axes.get_xaxis().set_visible(False)\n",
    "fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: frogs and distributions\n",
    "\n",
    "Our little CIFAR10 dataset contains 10 classes. One of those is frog. Look at these frogs and see what you notice about them? What strategies would you use if I told you to start drawing pictures of frogs that would fool someone trying to distinguish your drawings from these?\n",
    "\n",
    "Well, for one, most of the frogs are either green or brown blobs on top of a green or brown surface. But not all of them! If you truly wanted to fool the discriminator as well as possible, you wouldn't just want to be able to draw a frog well, you would want to be able to mimic the variety of frogs that the discriminator sees in the training data. If you never drew red frogs, then you would just make the discriminators job that much easier. \n",
    "\n",
    "So, because we've tasked the generator with fooling the discriminator, it won't do for the generator to just draw things well, it will have to model the *distribution of training data*.\n",
    "\n",
    "This is what makes GANs distinct from, say, a plain Autoencoder. The generator's ultimate goal is to learn the distribution of training dataâ€”in other words the entire world of possibilities that the training data came from. And in order to best the generator, the discriminator must also model that distribution. Once we're done, we have a model that can inform us about our data at deep level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the generator\n",
    "\n",
    "The structure of the generator is much like the discriminator in reverse. We'll start with a vector and use several layers to process that input. The successive layers will expand that input until it has the same width and height as the image. A final layer then shrinks the channels to the 3 color channels. We use `tanh` to squish the output to the interval `[-1, 1]`, which matches the preprocessing of our training data. \n",
    "\n",
    "```\n",
    "Dimensions      Layer Type\n",
    "----------      ----------\n",
    "[100]           Input \n",
    "                ... <preprocessing layers> ...\n",
    "[4 x 4 x 50]    Convolution (+ BN + upsampling + ConvTranspose) \n",
    "                ... <multiple conv layers> ...\n",
    "[32 x  32 x 50] Convolution\n",
    "[32 x 32 x 3]   Convolution (+ tanh)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrelu = functools.partial(K.relu, alpha=.1)\n",
    "z_len = 100\n",
    "\n",
    "conv = functools.partial(\n",
    "    keras.layers.Conv2D,\n",
    "    filters=50,\n",
    "    kernel_size=3,\n",
    "    padding=\"same\",\n",
    "    activation=lrelu,\n",
    "    strides=2,\n",
    ")\n",
    "\n",
    "disc = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(im_size, im_size, 3)),\n",
    "    conv(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    conv(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    conv(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "gen = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(z_len, )),\n",
    "    keras.layers.Dense(50 * 4 * 4, activation=lrelu),\n",
    "    keras.layers.Reshape((4, 4, 50)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.UpSampling2D(),\n",
    "    \n",
    "\n",
    "    conv(strides=1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.UpSampling2D(),\n",
    "    conv(strides=1),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.UpSampling2D(),\n",
    "\n",
    "    \n",
    "    conv(filters=3, strides=1, activation='tanh')\n",
    "])\n",
    "\n",
    "disc.summary()\n",
    "gen.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.compile(loss='binary_crossentropy',\n",
    "            optimizer=keras.optimizers.Adam(lr=0.0002, beta_1=.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator takes noise as input and generates imgs\n",
    "z = keras.Input(shape=(z_len,))\n",
    "img = gen(z)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "disc.trainable = False\n",
    "\n",
    "# The discriminator takes generated images as input and determines validity\n",
    "valid = disc(img)\n",
    "\n",
    "# The combined model  (stacked generator and discriminator)\n",
    "# Trains the generator to fool the discriminator\n",
    "comb = keras.Model(z, valid)\n",
    "comb.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0002, beta_1=.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "num_steps = x_train.shape[0]//batch_size\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "valid = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))\n",
    "\n",
    "d_loss_list = []\n",
    "g_loss_list = []\n",
    "\n",
    "# this will be used for sampling after each epoch. \n",
    "# if we stick with the same set of noise, we can see how the generated images changes\n",
    "\n",
    "noise_sample = np.random.normal(size=(sample_side**2, z_len))\n",
    "\n",
    "for epoch_i in range(num_epochs):\n",
    "    print(f\"starting epoch {epoch_i}\")\n",
    "    \n",
    "    for step_i in tqdm_notebook(range(num_steps)):\n",
    "        \n",
    "        idx = np.random.randint(0,x_train.shape[0], batch_size)\n",
    "        \n",
    "        x_batch = x_train[idx,...]\n",
    "        \n",
    "        noise_batch = np.random.normal(size=(batch_size, z_len))\n",
    "        \n",
    "        gen_batch = gen.predict(noise_batch)\n",
    "        \n",
    "        d_loss_valid = disc.train_on_batch(x_batch, valid)\n",
    "        d_loss_fake = disc.train_on_batch(gen_batch, fake)\n",
    "        d_loss_list.append(np.mean([d_loss_valid, d_loss_fake]))\n",
    "        \n",
    "        \n",
    "        g_loss = comb.train_on_batch(noise_batch, valid)\n",
    "        g_loss_list.append(g_loss)\n",
    "        \n",
    "    \n",
    "    \n",
    "    gen_sample = gen.predict(noise_sample)\n",
    "    \n",
    "    gen_sample = (gen_sample+1)/2\n",
    "    \n",
    "    gen_sample = gen_sample.reshape(sample_side,sample_side*im_size,im_size,3)\n",
    "    gen_sample = gen_sample.transpose((0,2,1,3))\n",
    "    gen_sample = gen_sample.reshape(sample_side*im_size,sample_side*im_size,3)\n",
    "    gen_sample = gen_sample.transpose((1,0,2))\n",
    "    \n",
    "    \n",
    "    plt.clf();\n",
    "    plt.figure();\n",
    "    plt.imshow(gen_sample)\n",
    "    plt.show();\n",
    "    \n",
    "    \n",
    "    print(f\" disc loss: {np.mean(d_loss_list[-num_steps:])} gen loss: {np.mean(g_loss_list[-num_steps:])}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
